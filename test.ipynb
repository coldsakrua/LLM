{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "s=5\n",
    "torch.tril(torch.ones(size=(s,s))).type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.Tensor([\n",
    "         [0., 0., 1., 1., 1.],\n",
    "         [1., 1., 1., 1., 1.]\n",
    "        ])\n",
    "\n",
    "torch.sum(attention_mask,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False, False, False],\n",
       "          [False, False, False, False, False],\n",
       "          [False, False,  True, False, False],\n",
       "          [False, False,  True,  True, False],\n",
       "          [False, False,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_attn=torch.sum(attention_mask,dim=1,dtype=torch.uint8)\n",
    "mask=torch.zeros(size=(attention_mask.shape[0],1,s,s),dtype=torch.bool)\n",
    "for b in range(attention_mask.shape[0]):\n",
    "    mask[b,0,s-tmp_attn[b]:,s-tmp_attn[b]:]=torch.tril(torch.ones(size=(tmp_attn[b],tmp_attn[b])))\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mha = MultiHeadAttention(d, h, 0.0)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m b, s, d, h, hd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhw1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiHeadAttention\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32me:\\slider and homework\\202509\\LLM\\hw1\\src\\model.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m count_params\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mDimension symbols:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    B - batch size\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    V - size of the vocabulary\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# mha = MultiHeadAttention(d, h, 0.0)\n",
    "b, s, d, h, hd = 16, 128, 768, 12, 64\n",
    "from hw1.src.model import MultiHeadAttention\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mha = MultiHeadAttention(d, h, 0.0)\n",
    "x = torch.rand(b, s, d)\n",
    "q, kT, v = mha.q_kT_v(x)\n",
    "k = rearrange(kT, \"b h hd s -> b h s hd\")\n",
    "\n",
    "attn = mha.self_attention(q, kT, v)\n",
    "attn_ref_multihead = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "attn_ref = rearrange(attn_ref_multihead, \"b h s hd -> b s (h hd)\")\n",
    "\n",
    "assert torch.allclose(attn, attn_ref, atol=1e-5, rtol=1e-3)\n",
    "\n",
    "x = torch.rand(2, 5, d)\n",
    "q, kT, v = mha.q_kT_v(x)\n",
    "k = rearrange(kT, \"b h hd s -> b h s hd\")\n",
    "attention_mask = torch.tensor(\n",
    "        [[0.0, 0.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0]]\n",
    "    )\n",
    "attention_mask_with_causal = torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                [False, False, False, False, False],\n",
    "                [False, False, False, False, False],\n",
    "                [False, False, True, False, False],\n",
    "                [False, False, True, True, False],\n",
    "                [False, False, True, True, True],\n",
    "            ],\n",
    "            [\n",
    "                [True, False, False, False, False],\n",
    "                [True, True, False, False, False],\n",
    "                [True, True, True, False, False],\n",
    "                [True, True, True, True, False],\n",
    "                [True, True, True, True, True],\n",
    "            ],\n",
    "        ]\n",
    "    )[:, None]\n",
    "attn = mha.self_attention(q, kT, v, attention_mask=attention_mask)\n",
    "attn_ref_multihead = F.scaled_dot_product_attention(\n",
    "        q, k, v, attn_mask=attention_mask_with_causal\n",
    "    )\n",
    "attn_ref = rearrange(attn_ref_multihead, \"b h s hd -> b s (h hd)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "position_ids = torch.Tensor([\n",
    "         [0, 0, 0, 1, 2],\n",
    "         [0, 1, 2, 3, 4]\n",
    "        ]).type(torch.long)\n",
    "\n",
    "class a(nn.Module):\n",
    "        def __init__(self):\n",
    "                super().__init__()\n",
    "                self.position_embeddings = nn.Embedding(5, 10)\n",
    "        def forward(self,x):\n",
    "                return self.position_embeddings(x)\n",
    "        \n",
    "aa=a()\n",
    "aa(position_ids).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(5).unsqueeze(0).expand(12, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(\n",
    "    input_ids: torch.LongTensor, logits: torch.FloatTensor\n",
    ") -> torch.FloatTensor:\n",
    "    \"\"\"Outputs the language modeling loss given input_ids and logits\n",
    "\n",
    "    Args:\n",
    "        input_ids: the input token ids\n",
    "        logits: the next token logits produced by the language model\n",
    "\n",
    "    Returns:\n",
    "        loss: the mean cross entropy loss for next token prediction\n",
    "\n",
    "    Hint: Think about what are the groundtruth labels for next token prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = input_ids[:,1:]\n",
    "    logits = logits[:,:-1,:]\n",
    "\n",
    "    loss =  F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
